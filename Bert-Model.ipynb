{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04c2c6958f73f61d7b8bffd4193a0d9584c9ef39256921fe54ce430734ddbdc1d",
   "display_name": "Python 3.9.4 64-bit ('QandU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Weitergehende Sicherungsmaßnahmen können eine Videoüberwachung und eine Zugangskontrolle durch einen Türöffner sein, denn viele GAA befinden sich in Vorräumen der Geschäftsstellen der Banken, sodass sie auch außerhalb der Schalteröffnungszeiten zugänglich sind.'"
      ]
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "# load data and write out sentence and target\n",
    "import pandas as pd\n",
    "\n",
    "loaded_set = pd.read_excel(\"Dataset/\"+\"training.xlsx\")\n",
    "loaded_set['Sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Weiter',\n",
       " '##gehende',\n",
       " 'Sicherungs',\n",
       " '##maßnahmen',\n",
       " 'können',\n",
       " 'eine',\n",
       " 'Video',\n",
       " '##überwachung',\n",
       " 'und',\n",
       " 'eine',\n",
       " 'Zugangs',\n",
       " '##kontrolle',\n",
       " 'durch',\n",
       " 'einen',\n",
       " 'Tür',\n",
       " '##öff',\n",
       " '##ner',\n",
       " 'sein',\n",
       " ',',\n",
       " 'denn',\n",
       " 'viele',\n",
       " 'G',\n",
       " '##AA',\n",
       " 'befinden',\n",
       " 'sich',\n",
       " 'in',\n",
       " 'Vor',\n",
       " '##räumen',\n",
       " 'der',\n",
       " 'Geschäfts',\n",
       " '##stellen',\n",
       " 'der',\n",
       " 'Banken',\n",
       " ',',\n",
       " 'sodass',\n",
       " 'sie',\n",
       " 'auch',\n",
       " 'außerhalb',\n",
       " 'der',\n",
       " 'Schalter',\n",
       " '##öffnungs',\n",
       " '##zeiten',\n",
       " 'zugänglich',\n",
       " 'sind',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "tokens_num=[]\n",
    "for sen in loaded_set['Sentence']:\n",
    "    tokenized = (tokenizer.tokenize(sen)) \n",
    "    tokens_num.append( ['[CLS]'] + tokenized + ['[SEP]']) \n",
    "    \n",
    "# get max_seq length    \n",
    "lens = [len(i) for i in tokens_num]\n",
    "max_seq_length = max(lens)\n",
    "max_seq_length = int(1.5*max_seq_length)\n",
    "tokens_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[102,\n",
       " 1784,\n",
       " 13183,\n",
       " 28847,\n",
       " 4686,\n",
       " 618,\n",
       " 261,\n",
       " 4770,\n",
       " 20815,\n",
       " 136,\n",
       " 261,\n",
       " 21093,\n",
       " 11600,\n",
       " 387,\n",
       " 397,\n",
       " 2451,\n",
       " 706,\n",
       " 432,\n",
       " 290,\n",
       " 818,\n",
       " 1398,\n",
       " 1358,\n",
       " 159,\n",
       " 10695,\n",
       " 3857,\n",
       " 251,\n",
       " 153,\n",
       " 445,\n",
       " 7721,\n",
       " 125,\n",
       " 2484,\n",
       " 984,\n",
       " 125,\n",
       " 8232,\n",
       " 818,\n",
       " 7415,\n",
       " 307,\n",
       " 313,\n",
       " 5729,\n",
       " 125,\n",
       " 28802,\n",
       " 17893,\n",
       " 4083,\n",
       " 10370,\n",
       " 341,\n",
       " 566,\n",
       " 103]"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens_num[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# german tokens for bert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "#model = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list, tokenizer, max_seq_length):\n",
    "  num_examples = len(string_list)\n",
    "  \n",
    "  string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "  input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_tokens = tf.ones_like(string_tokens)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49     2.30\n",
       "863    2.11\n",
       "607    4.07\n",
       "853    2.62\n",
       "74     3.29\n",
       "       ... \n",
       "88     1.89\n",
       "892    1.31\n",
       "310    3.20\n",
       "555    2.11\n",
       "727    4.00\n",
       "Name: MOS, Length: 720, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "x = loaded_set['Sentence']\n",
    "y = loaded_set['MOS']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=32)\n",
    "y_train = round(y_train, 2)\n",
    "y_test = round(y_test, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(144,), dtype=int32, numpy=\n",
       "array([  102,  2135,   153,   128,  5500,   105,   744,   136,  3994,\n",
       "         285,   128, 13517,  6026,   125,  9348,   809,   313,   153,\n",
       "         125, 23568,  4030,  3627,   809,   842,   222, 23306,  2178,\n",
       "        1579,  1084,   566,   103,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0])>"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "\n",
    "X_train = bert_encode(x_train, tokenizer, max_seq_length)\n",
    "X_test = bert_encode(x_test, tokenizer, max_seq_length)\n",
    "X_train['input_word_ids'][80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_class = 1  # Based on available class selection\n",
    "max_seq_length = max_seq_length  # we calculated this a couple cells ago\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "#output_drop = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "output_dense_1 = tf.keras.layers.Dense(256, activation='relu', name='dense_1')(pooled_output)\n",
    "output_dense_2 = tf.keras.layers.Dense(128, activation='relu', name='dense_2')(output_dense_1)\n",
    "output_dense_3 = tf.keras.layers.Dense(128, activation='relu', name='dense_3')(output_dense_2)\n",
    "output = tf.keras.layers.Dense(1, activation='softmax', name='output')(output_dense_3)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs={\n",
    "        'input_word_ids':input_word_ids,\n",
    "        'input_mask':input_mask,\n",
    "        'input_type_ids':segment_ids\n",
    "    },\n",
    "    outputs = output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 8  # select based on your GPU resources\n",
    "\n",
    "\n",
    "train_data_size = len(y_train)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "            loss='mse'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_11\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 144)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 144)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 144)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer_10 (KerasLayer)     [(None, 768), (None, 177853441   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          196864      keras_layer_10[0][0]             \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          16512       dense_2[0][0]                    \n__________________________________________________________________________________________________\noutput (Dense)                  (None, 1)            129         dense_3[0][0]                    \n==================================================================================================\nTotal params: 178,099,842\nTrainable params: 178,099,841\nNon-trainable params: 1\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "90/90 [==============================] - 709s 8s/step - loss: 5.5403 - val_loss: 5.0111\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 653s 7s/step - loss: 5.5403 - val_loss: 5.0111\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 636s 7s/step - loss: 5.5403 - val_loss: 5.0111\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 640s 7s/step - loss: 5.5404 - val_loss: 5.0111\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 638s 7s/step - loss: 5.5403 - val_loss: 5.0111\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "rounded_pred = np.around(pred, decimals=2)\n",
    "rounded_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.238544045678897"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "rmse(rounded_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}