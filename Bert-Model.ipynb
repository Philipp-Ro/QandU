{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04c2c6958f73f61d7b8bffd4193a0d9584c9ef39256921fe54ce430734ddbdc1d",
   "display_name": "Python 3.9.4 64-bit ('QandU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      Weitergehende Sicherungsmaßnahmen können eine ...\n",
       "1      In der großen Kasernenanlage im Norden Kiels k...\n",
       "2      Premierminister David Lloyd George honorierte ...\n",
       "3      Der Beitrag der Truppen dieser Dominions währe...\n",
       "4      Eine Balance zwischen verschiedenen Lebensbere...\n",
       "                             ...                        \n",
       "895    Zwischen 1901 und 2010 ist er um ca 1,7 cm pro...\n",
       "896    Aus Furcht vor einem Bürgerkrieg wollte sie – ...\n",
       "897    In den meisten Politikfeldern gilt dafür seit ...\n",
       "898    Aufgrund der Wärmekapazität des Gesteins, und ...\n",
       "899    Die Klinge ist zumeist aus nicht rostfreiem Ko...\n",
       "Name: Sentence, Length: 900, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "# load data and write out sentence and target\n",
    "import pandas as pd\n",
    "\n",
    "loaded_set = pd.read_excel(\"Dataset/\"+\"training.xlsx\")\n",
    "\n",
    "#x_train, x_test,  y_train, y_test = train_test_split(loaded_set['Sentence'], loaded_set['MOS'], test_size=.2)\n",
    "loaded_set['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "tokens_num=[]\n",
    "for sen in loaded_set['Sentence']:\n",
    "    tokenized = (tokenizer.tokenize(sen)) \n",
    "    tokens_num.append( ['[CLS]'] + tokenized + ['[SEP]']) \n",
    "    \n",
    "# get max_seq length    \n",
    "lens = [len(i) for i in tokens_num]\n",
    "max_seq_length = max(lens)\n",
    "max_seq_length = int(1.5*max_seq_length)\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# german tokens for bert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "#model = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list, tokenizer, max_seq_length):\n",
    "  num_examples = len(string_list)\n",
    "  \n",
    "  string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "  input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_tokens = tf.ones_like(string_tokens)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49     2.30\n",
       "863    2.11\n",
       "607    4.07\n",
       "853    2.62\n",
       "74     3.29\n",
       "       ... \n",
       "88     1.89\n",
       "892    1.31\n",
       "310    3.20\n",
       "555    2.11\n",
       "727    4.00\n",
       "Name: MOS, Length: 720, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "x = loaded_set['Sentence']\n",
    "y = loaded_set['MOS']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=32)\n",
    "y_train = round(y_train, 2)\n",
    "y_test = round(y_test, 2)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = bert_encode(x_train, tokenizer, max_seq_length)\n",
    "X_test = bert_encode(x_test, tokenizer, max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a 1d CNN\n",
    "import tensorflow as tf\n",
    "num_class = 1  # Based on available class selection\n",
    "max_seq_length = max_seq_length  # we calculated this a couple cells ago\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "output_drop = tf.keras.layers.Dropout(rate=0.1)(sequence_output)\n",
    "output_dense_1 = tf.keras.layers.Dense(20, activation='relu', name='dense_1')(output_drop)\n",
    "output = tf.keras.layers.Dense(1, activation='softmax', name='output')(output_dense_1)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs={\n",
    "        'input_word_ids':input_word_ids,\n",
    "        'input_mask':input_mask,\n",
    "        'input_type_ids':segment_ids\n",
    "    },\n",
    "    outputs = output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 8  # select based on your GPU resources\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "train_data_size = len(y_train)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['accuracy']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_8\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 144)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 144)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 144)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer_7 (KerasLayer)      [(None, 768), (None, 177853441   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, None, 768)    0           keras_layer_7[0][1]              \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, None, 20)     15380       dropout_8[0][0]                  \n__________________________________________________________________________________________________\noutput (Dense)                  (None, None, 1)      21          dense_1[0][0]                    \n==================================================================================================\nTotal params: 177,868,842\nTrainable params: 177,868,841\nNon-trainable params: 1\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "90/90 [==============================] - 637s 7s/step - loss: 5.5403 - accuracy: 0.0611 - val_loss: 5.0111 - val_accuracy: 0.1111\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}