{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04c2c6958f73f61d7b8bffd4193a0d9584c9ef39256921fe54ce430734ddbdc1d",
   "display_name": "Python 3.9.4 64-bit ('QandU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Weitergehende Sicherungsmaßnahmen können eine Videoüberwachung und eine Zugangskontrolle durch einen Türöffner sein, denn viele GAA befinden sich in Vorräumen der Geschäftsstellen der Banken, sodass sie auch außerhalb der Schalteröffnungszeiten zugänglich sind.'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# load data and write out sentence and target\n",
    "import pandas as pd\n",
    "\n",
    "loaded_set = pd.read_excel(\"Dataset/\"+\"training.xlsx\")\n",
    "loaded_set['Sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Weiter',\n",
       " '##gehende',\n",
       " 'Sicherungs',\n",
       " '##maßnahmen',\n",
       " 'können',\n",
       " 'eine',\n",
       " 'Video',\n",
       " '##überwachung',\n",
       " 'und',\n",
       " 'eine',\n",
       " 'Zugangs',\n",
       " '##kontrolle',\n",
       " 'durch',\n",
       " 'einen',\n",
       " 'Tür',\n",
       " '##öff',\n",
       " '##ner',\n",
       " 'sein',\n",
       " ',',\n",
       " 'denn',\n",
       " 'viele',\n",
       " 'G',\n",
       " '##AA',\n",
       " 'befinden',\n",
       " 'sich',\n",
       " 'in',\n",
       " 'Vor',\n",
       " '##räumen',\n",
       " 'der',\n",
       " 'Geschäfts',\n",
       " '##stellen',\n",
       " 'der',\n",
       " 'Banken',\n",
       " ',',\n",
       " 'sodass',\n",
       " 'sie',\n",
       " 'auch',\n",
       " 'außerhalb',\n",
       " 'der',\n",
       " 'Schalter',\n",
       " '##öffnungs',\n",
       " '##zeiten',\n",
       " 'zugänglich',\n",
       " 'sind',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# german tokens for bert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "#model = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "\n",
    "\n",
    "\n",
    "tokens_num=[]\n",
    "for sen in loaded_set['Sentence']:\n",
    "    tokenized = (tokenizer.tokenize(sen)) \n",
    "    tokens_num.append( ['[CLS]'] + tokenized + ['[SEP]']) \n",
    "    \n",
    "# get max_seq length    \n",
    "lens = [len(i) for i in tokens_num]\n",
    "max_seq_length = max(lens)\n",
    "max_seq_length = int(1.5*max_seq_length)\n",
    "tokens_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[102,\n",
       " 1784,\n",
       " 13183,\n",
       " 28847,\n",
       " 4686,\n",
       " 618,\n",
       " 261,\n",
       " 4770,\n",
       " 20815,\n",
       " 136,\n",
       " 261,\n",
       " 21093,\n",
       " 11600,\n",
       " 387,\n",
       " 397,\n",
       " 2451,\n",
       " 706,\n",
       " 432,\n",
       " 290,\n",
       " 818,\n",
       " 1398,\n",
       " 1358,\n",
       " 159,\n",
       " 10695,\n",
       " 3857,\n",
       " 251,\n",
       " 153,\n",
       " 445,\n",
       " 7721,\n",
       " 125,\n",
       " 2484,\n",
       " 984,\n",
       " 125,\n",
       " 8232,\n",
       " 818,\n",
       " 7415,\n",
       " 307,\n",
       " 313,\n",
       " 5729,\n",
       " 125,\n",
       " 28802,\n",
       " 17893,\n",
       " 4083,\n",
       " 10370,\n",
       " 341,\n",
       " 566,\n",
       " 103]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens_num[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "def encode_names(n, tokenizer):\n",
    "   tokens = list(tokenizer.tokenize(n))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list, tokenizer, max_seq_length):\n",
    "  num_examples = len(string_list)\n",
    "  \n",
    "  string_tokens = tf.ragged.constant([\n",
    "      encode_names(n, tokenizer) for n in np.array(string_list)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*string_tokens.shape[0]\n",
    "  input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_tokens = tf.ones_like(string_tokens)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "  scaler_input_word_ids = scaler.fit_transform(input_type_ids)  \n",
    "\n",
    "  inputs = {\n",
    "      #'sc': scaler_input_word_ids,\n",
    "      #'input_word_ids': input_word_ids,\n",
    "      'input_word_ids': input_word_ids.to_tensor(shape=(None, max_seq_length)),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49     2.30\n",
       "863    2.11\n",
       "607    4.07\n",
       "853    2.62\n",
       "74     3.29\n",
       "       ... \n",
       "88     1.89\n",
       "892    1.31\n",
       "310    3.20\n",
       "555    2.11\n",
       "727    4.00\n",
       "Name: MOS, Length: 720, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = loaded_set['Sentence']\n",
    "y = loaded_set['MOS']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=32)\n",
    "y_train = round(y_train, 2)\n",
    "y_test = round(y_test, 2)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(720, 144), dtype=int32, numpy=\n",
       "array([[  102, 24507,  3518, ...,     0,     0,     0],\n",
       "       [  102,   465,   125, ...,     0,     0,     0],\n",
       "       [  102,   596,  3180, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  102,   396,   125, ...,     0,     0,     0],\n",
       "       [  102,  1130,  3484, ...,     0,     0,     0],\n",
       "       [  102,   259,  2409, ...,     0,     0,     0]])>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "X_train = bert_encode(x_train, tokenizer, max_seq_length)\n",
    "X_test = bert_encode(x_test, tokenizer, max_seq_length)\n",
    "X_train['input_word_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(720, 1) (720, 144, 1) (180, 144, 1) (180, 1)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[ 0.        ],\n",
       "        [ 3.45053811],\n",
       "        [-0.34845038],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [-0.52441672],\n",
       "        [-0.76153954],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [-0.50275799],\n",
       "        [-0.38960103],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [-0.53582475],\n",
       "        [-0.76153954],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [-0.41446975],\n",
       "        [-0.3525898 ],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]],\n",
       "\n",
       "       [[ 0.        ],\n",
       "        [-0.55847548],\n",
       "        [-0.48346832],\n",
       "        ...,\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train['input_word_ids'] = scaler.fit_transform(X_train['input_word_ids'])\n",
    "X_test['input_word_ids'] = scaler.transform(X_test['input_word_ids'])\n",
    "n_timesteps = len(X_train['input_word_ids'])\n",
    "n_features = len(X_train['input_word_ids'][0])\n",
    "trainX = tf.reshape(X_train['input_word_ids'], [n_timesteps, n_features, 1]).numpy()\n",
    "trainy = tf.reshape(y_train, [n_timesteps, 1]).numpy()\n",
    "\n",
    "testX = tf.reshape(X_test['input_word_ids'], [len(X_test['input_word_ids']), n_features, 1]).numpy()\n",
    "testy = tf.reshape(y_test, [len(X_test['input_word_ids']), 1]).numpy()\n",
    "print(trainy.shape, trainX.shape, testX.shape, testy.shape)\n",
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_4 (Conv1D)            (None, 142, 64)           256       \n_________________________________________________________________\nconv1d_5 (Conv1D)            (None, 140, 32)           6176      \n_________________________________________________________________\ndense_3 (Dense)              (None, 140, 258)          8514      \n_________________________________________________________________\ndropout (Dropout)            (None, 140, 258)          0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 140, 1)            259       \n=================================================================\nTotal params: 15,205\nTrainable params: 15,205\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "model_conv = tf.keras.Sequential()\n",
    "model_conv.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "model_conv.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_conv.add(tf.keras.layers.Dense(258, activation='relu'))\n",
    "model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "model_conv.add(tf.keras.layers.Dense(n_outputs, activation='softmax'))\n",
    "model_conv.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.0544\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 2.0544\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 2.0544\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.0544\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.0544\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0544\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0544\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0544\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0544\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0544\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.0544\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0544\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0544\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x228bbd12cd0>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "epochs =  20\n",
    "batch_size = 12 \n",
    "verbose = 1\n",
    "model_conv.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 8  # select based on your GPU resources\n",
    "\n",
    "\n",
    "train_data_size = len(y_train)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "            loss='mse'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 144)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 144)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 144)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer_1 (KerasLayer)      [(None, 768), (None, 177853441   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          196864      keras_layer_1[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          16512       dense_2[0][0]                    \n__________________________________________________________________________________________________\noutput (Dense)                  (None, 1)            129         dense_3[0][0]                    \n==================================================================================================\nTotal params: 178,099,842\nTrainable params: 178,099,841\nNon-trainable params: 1\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "90/90 [==============================] - 521s 6s/step - loss: 5.5403 - val_loss: 5.0111\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "pred = model_conv.predict(testX)\n",
    "rounded_pred = np.around(pred, decimals=2)\n",
    "rounded_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.238544045678897"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "rmse(rounded_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}