{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04c2c6958f73f61d7b8bffd4193a0d9584c9ef39256921fe54ce430734ddbdc1d",
   "display_name": "Python 3.9.4 64-bit ('QandU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      Weitergehende Sicherungsmaßnahmen können eine ...\n",
       "1      In der großen Kasernenanlage im Norden Kiels k...\n",
       "2      Premierminister David Lloyd George honorierte ...\n",
       "3      Der Beitrag der Truppen dieser Dominions währe...\n",
       "4      Eine Balance zwischen verschiedenen Lebensbere...\n",
       "                             ...                        \n",
       "895    Zwischen 1901 und 2010 ist er um ca 1,7 cm pro...\n",
       "896    Aus Furcht vor einem Bürgerkrieg wollte sie – ...\n",
       "897    In den meisten Politikfeldern gilt dafür seit ...\n",
       "898    Aufgrund der Wärmekapazität des Gesteins, und ...\n",
       "899    Die Klinge ist zumeist aus nicht rostfreiem Ko...\n",
       "Name: Sentence, Length: 900, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "# load data and write out sentence and target\n",
    "import pandas as pd\n",
    "\n",
    "loaded_set = pd.read_excel(\"Dataset/\"+\"training.xlsx\")\n",
    "\n",
    "#x_train, x_test,  y_train, y_test = train_test_split(loaded_set['Sentence'], loaded_set['MOS'], test_size=.2)\n",
    "loaded_set['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  akkusativ  genitiv  num_words  letter_count  \\\n",
       "0        1          0        0         31           231   \n",
       "1        1          0        0         11            61   \n",
       "2        1          1        0         28           204   \n",
       "3        1          0        0         24           139   \n",
       "4        1          0        0         36           297   \n",
       "..     ...        ...      ...        ...           ...   \n",
       "895      1          0        0         27           114   \n",
       "896      1          1        0         29           180   \n",
       "897      1          1        0         43           285   \n",
       "898      1          0        1         32           211   \n",
       "899      1          0        0         12            81   \n",
       "\n",
       "     avarange_letter_per_word  longest_word_length  shortest_word_length  \n",
       "0                    7.451613                   22                     2  \n",
       "1                    5.545455                   16                     2  \n",
       "2                    7.285714                   20                     2  \n",
       "3                    5.791667                   11                     3  \n",
       "4                    8.250000                   23                     2  \n",
       "..                        ...                  ...                   ...  \n",
       "895                  4.222222                   10                     2  \n",
       "896                  6.206897                   14                     1  \n",
       "897                  6.627907                   22                     2  \n",
       "898                  6.593750                   15                     3  \n",
       "899                  6.750000                   16                     3  \n",
       "\n",
       "[900 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>akkusativ</th>\n      <th>genitiv</th>\n      <th>num_words</th>\n      <th>letter_count</th>\n      <th>avarange_letter_per_word</th>\n      <th>longest_word_length</th>\n      <th>shortest_word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31</td>\n      <td>231</td>\n      <td>7.451613</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>61</td>\n      <td>5.545455</td>\n      <td>16</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28</td>\n      <td>204</td>\n      <td>7.285714</td>\n      <td>20</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24</td>\n      <td>139</td>\n      <td>5.791667</td>\n      <td>11</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>297</td>\n      <td>8.250000</td>\n      <td>23</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>114</td>\n      <td>4.222222</td>\n      <td>10</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>29</td>\n      <td>180</td>\n      <td>6.206897</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>897</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>43</td>\n      <td>285</td>\n      <td>6.627907</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>32</td>\n      <td>211</td>\n      <td>6.593750</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>899</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>81</td>\n      <td>6.750000</td>\n      <td>16</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "letter_count = []\n",
    "avarange_letter_per_word = []\n",
    "num_words = []\n",
    "num_letters_array = []\n",
    "longest_word_length = []\n",
    "shortest_word_length = []\n",
    "genitiv = []\n",
    "akkusativ = []\n",
    "dativ =[]\n",
    "\n",
    "for sen in loaded_set['Sentence']:\n",
    "    current_sen_split = sen.split()\n",
    "    num_words.append(len(current_sen_split))\n",
    "    num_letters = []\n",
    "        \n",
    "    if \"des\" in sen:\n",
    "        genitiv.append(1)\n",
    "    else:\n",
    "        genitiv.append(0)\n",
    "\n",
    "    if \"dem\" in sen:\n",
    "        akkusativ.append(1)\n",
    "    else:\n",
    "        akkusativ.append(0)\n",
    "\n",
    "    if \"den\" in sen:\n",
    "        dativ.append(1)\n",
    "    else:\n",
    "        dativ.append(1)\n",
    "\n",
    "    for y in range(len(current_sen_split)):\n",
    "        current_word = current_sen_split[y]\n",
    "   \n",
    "        num_letters.append(len(current_word))\n",
    "   \n",
    "    current_lettercount = sum(num_letters)\n",
    "    letter_count.append(current_lettercount) \n",
    "    avarange_letter_per_word.append(current_lettercount/len(current_sen_split))\n",
    "    longest_word_length.append(max(num_letters)) \n",
    "    shortest_word_length.append(min(num_letters)) \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "feature_dict = {\n",
    "    'dativ':dativ, \n",
    "    'akkusativ': akkusativ, \n",
    "    'genitiv': genitiv, \n",
    "    'num_words':num_words,\n",
    "    'letter_count':letter_count,\n",
    "    'avarange_letter_per_word':avarange_letter_per_word,\n",
    "    'longest_word_length':longest_word_length,\n",
    "    'shortest_word_length':shortest_word_length, \n",
    "    }\n",
    "feature_dataframe = pd.DataFrame(data=feature_dict)\n",
    "feature_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "498    2.78\n",
       "585    1.00\n",
       "294    2.00\n",
       "366    3.13\n",
       "561    3.25\n",
       "       ... \n",
       "394    3.20\n",
       "27     3.14\n",
       "625    2.18\n",
       "147    2.33\n",
       "793    1.00\n",
       "Name: MOS, Length: 720, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_dataframe\n",
    "y = round(loaded_set['MOS'],2)\n",
    "\n",
    "x_train, x_test,  y_train, y_test = train_test_split(X, y, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-131-aa75ddb38d9b>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.fit_transform(x_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n<ipython-input-131-aa75ddb38d9b>:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.transform(x_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  akkusativ  genitiv  num_words  letter_count  \\\n",
       "10       1          0        0  -0.009228      0.298044   \n",
       "749      1          0        0   0.458655      0.675309   \n",
       "726      1          0        0   0.084349      0.283533   \n",
       "643      1          0        0  -0.757840     -0.616099   \n",
       "619      1          1        0   1.020114      1.487881   \n",
       "..     ...        ...      ...        ...           ...   \n",
       "554      1          0        0  -0.851416     -0.659630   \n",
       "204      1          0        0  -0.102804     -0.311385   \n",
       "520      1          0        0   2.985221      2.924391   \n",
       "683      1          0        0   1.487997      2.445554   \n",
       "839      1          0        0   0.645808      0.225493   \n",
       "\n",
       "     avarange_letter_per_word  longest_word_length  shortest_word_length  \n",
       "10                   1.017806             1.096541              1.254460  \n",
       "749                  0.567240             1.336952             -0.330121  \n",
       "726                  0.614919             1.336952             -0.330121  \n",
       "643                  0.734116             0.134897             -0.330121  \n",
       "619                  1.019421             1.577362             -0.330121  \n",
       "..                        ...                  ...                   ...  \n",
       "554                  1.113381             0.615719              1.254460  \n",
       "204                 -0.802905            -1.067157              1.254460  \n",
       "520                 -0.087430             0.375308             -0.330121  \n",
       "683                  1.818814             1.817773             -0.330121  \n",
       "839                 -1.101526             1.336952             -0.330121  \n",
       "\n",
       "[180 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>akkusativ</th>\n      <th>genitiv</th>\n      <th>num_words</th>\n      <th>letter_count</th>\n      <th>avarange_letter_per_word</th>\n      <th>longest_word_length</th>\n      <th>shortest_word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.009228</td>\n      <td>0.298044</td>\n      <td>1.017806</td>\n      <td>1.096541</td>\n      <td>1.254460</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.458655</td>\n      <td>0.675309</td>\n      <td>0.567240</td>\n      <td>1.336952</td>\n      <td>-0.330121</td>\n    </tr>\n    <tr>\n      <th>726</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.084349</td>\n      <td>0.283533</td>\n      <td>0.614919</td>\n      <td>1.336952</td>\n      <td>-0.330121</td>\n    </tr>\n    <tr>\n      <th>643</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.757840</td>\n      <td>-0.616099</td>\n      <td>0.734116</td>\n      <td>0.134897</td>\n      <td>-0.330121</td>\n    </tr>\n    <tr>\n      <th>619</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1.020114</td>\n      <td>1.487881</td>\n      <td>1.019421</td>\n      <td>1.577362</td>\n      <td>-0.330121</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>554</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.851416</td>\n      <td>-0.659630</td>\n      <td>1.113381</td>\n      <td>0.615719</td>\n      <td>1.254460</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.102804</td>\n      <td>-0.311385</td>\n      <td>-0.802905</td>\n      <td>-1.067157</td>\n      <td>1.254460</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.985221</td>\n      <td>2.924391</td>\n      <td>-0.087430</td>\n      <td>0.375308</td>\n      <td>-0.330121</td>\n    </tr>\n    <tr>\n      <th>683</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.487997</td>\n      <td>2.445554</td>\n      <td>1.818814</td>\n      <td>1.817773</td>\n      <td>-0.330121</td>\n    </tr>\n    <tr>\n      <th>839</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.645808</td>\n      <td>0.225493</td>\n      <td>-1.101526</td>\n      <td>1.336952</td>\n      <td>-0.330121</td>\n    </tr>\n  </tbody>\n</table>\n<p>180 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.fit_transform(x_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\n",
    "\n",
    "X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.transform(x_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\n",
    "\n",
    "X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_33 (Dense)             (None, 64)                576       \n_________________________________________________________________\ndense_34 (Dense)             (None, 64)                4160      \n_________________________________________________________________\ndense_35 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndense_36 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 6,849\nTrainable params: 6,849\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.InputLayer(8),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "360/360 [==============================] - 2s 2ms/step - loss: 0.9322 - val_loss: 0.6745\n",
      "Epoch 2/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.6417 - val_loss: 0.6029\n",
      "Epoch 3/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.6364 - val_loss: 0.5654\n",
      "Epoch 4/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.6035 - val_loss: 0.5789\n",
      "Epoch 5/100\n",
      "360/360 [==============================] - 1s 1ms/step - loss: 0.6101 - val_loss: 0.5965\n",
      "Epoch 6/100\n",
      "360/360 [==============================] - 1s 1ms/step - loss: 0.6016 - val_loss: 0.5577\n",
      "Epoch 7/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5936 - val_loss: 0.5674\n",
      "Epoch 8/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5908 - val_loss: 0.6257\n",
      "Epoch 9/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5920 - val_loss: 0.5640\n",
      "Epoch 10/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5780 - val_loss: 0.5464\n",
      "Epoch 11/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5722 - val_loss: 0.5471\n",
      "Epoch 12/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5906 - val_loss: 0.5547\n",
      "Epoch 13/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5754 - val_loss: 0.5476\n",
      "Epoch 14/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5559 - val_loss: 0.6022\n",
      "Epoch 15/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5836 - val_loss: 0.5766\n",
      "Epoch 16/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5695 - val_loss: 0.5357\n",
      "Epoch 17/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5613 - val_loss: 0.5380\n",
      "Epoch 18/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5802 - val_loss: 0.5634\n",
      "Epoch 19/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5598 - val_loss: 0.5698\n",
      "Epoch 20/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5702 - val_loss: 0.5593\n",
      "Epoch 21/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5651 - val_loss: 0.5570\n",
      "Epoch 22/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5720 - val_loss: 0.5640\n",
      "Epoch 23/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5588 - val_loss: 0.5655\n",
      "Epoch 24/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5493 - val_loss: 0.7128\n",
      "Epoch 25/100\n",
      "360/360 [==============================] - 1s 1ms/step - loss: 0.5636 - val_loss: 0.5492\n",
      "Epoch 26/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.5579\n",
      "Epoch 27/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5580 - val_loss: 0.5579\n",
      "Epoch 28/100\n",
      "360/360 [==============================] - 0s 945us/step - loss: 0.5469 - val_loss: 0.5633\n",
      "Epoch 29/100\n",
      "360/360 [==============================] - 0s 878us/step - loss: 0.5432 - val_loss: 0.5609\n",
      "Epoch 30/100\n",
      "360/360 [==============================] - 0s 900us/step - loss: 0.5451 - val_loss: 0.5577\n",
      "Epoch 31/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5398 - val_loss: 0.5845\n",
      "Epoch 32/100\n",
      "360/360 [==============================] - 1s 1ms/step - loss: 0.5382 - val_loss: 0.6096\n",
      "Epoch 33/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5541 - val_loss: 0.5663\n",
      "Epoch 34/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5328 - val_loss: 0.5853\n",
      "Epoch 35/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5634\n",
      "Epoch 36/100\n",
      "360/360 [==============================] - 0s 892us/step - loss: 0.5250 - val_loss: 0.5509\n",
      "Epoch 37/100\n",
      "360/360 [==============================] - 0s 978us/step - loss: 0.5326 - val_loss: 0.5889\n",
      "Epoch 38/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5358 - val_loss: 0.5768\n",
      "Epoch 39/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5149 - val_loss: 0.6006\n",
      "Epoch 40/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 0.5879\n",
      "Epoch 41/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5233 - val_loss: 0.5868\n",
      "Epoch 42/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5288 - val_loss: 0.5841\n",
      "Epoch 43/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5291 - val_loss: 0.5929\n",
      "Epoch 44/100\n",
      "360/360 [==============================] - 0s 975us/step - loss: 0.5266 - val_loss: 0.6366\n",
      "Epoch 45/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5700\n",
      "Epoch 46/100\n",
      "360/360 [==============================] - 0s 889us/step - loss: 0.5113 - val_loss: 0.5945\n",
      "Epoch 47/100\n",
      "360/360 [==============================] - 0s 961us/step - loss: 0.5158 - val_loss: 0.5881\n",
      "Epoch 48/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5151 - val_loss: 0.5780\n",
      "Epoch 49/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5106 - val_loss: 0.5627\n",
      "Epoch 50/100\n",
      "360/360 [==============================] - 1s 1ms/step - loss: 0.5066 - val_loss: 0.5962\n",
      "Epoch 51/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5004 - val_loss: 0.6075\n",
      "Epoch 52/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5116 - val_loss: 0.5702\n",
      "Epoch 53/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5055 - val_loss: 0.5940\n",
      "Epoch 54/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.5028 - val_loss: 0.5815\n",
      "Epoch 55/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5070 - val_loss: 0.5728\n",
      "Epoch 56/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.5000 - val_loss: 0.6228\n",
      "Epoch 57/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4978 - val_loss: 0.5873\n",
      "Epoch 58/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4991 - val_loss: 0.6069\n",
      "Epoch 59/100\n",
      "360/360 [==============================] - 0s 931us/step - loss: 0.4856 - val_loss: 0.6257\n",
      "Epoch 60/100\n",
      "360/360 [==============================] - 0s 894us/step - loss: 0.5012 - val_loss: 0.6060\n",
      "Epoch 61/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4929 - val_loss: 0.6324\n",
      "Epoch 62/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4883 - val_loss: 0.6008\n",
      "Epoch 63/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4907 - val_loss: 0.5914\n",
      "Epoch 64/100\n",
      "360/360 [==============================] - 0s 964us/step - loss: 0.4851 - val_loss: 0.5845\n",
      "Epoch 65/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4772 - val_loss: 0.6084\n",
      "Epoch 66/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4808 - val_loss: 0.6080\n",
      "Epoch 67/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4811 - val_loss: 0.6306\n",
      "Epoch 68/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4839 - val_loss: 0.6112\n",
      "Epoch 69/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4820 - val_loss: 0.6133\n",
      "Epoch 70/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4814 - val_loss: 0.5949\n",
      "Epoch 71/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4723 - val_loss: 0.5984\n",
      "Epoch 72/100\n",
      "360/360 [==============================] - 0s 928us/step - loss: 0.4717 - val_loss: 0.6463\n",
      "Epoch 73/100\n",
      "360/360 [==============================] - 0s 900us/step - loss: 0.4703 - val_loss: 0.6120\n",
      "Epoch 74/100\n",
      "360/360 [==============================] - 0s 894us/step - loss: 0.4754 - val_loss: 0.6092\n",
      "Epoch 75/100\n",
      "360/360 [==============================] - 0s 961us/step - loss: 0.4670 - val_loss: 0.6125\n",
      "Epoch 76/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.6169\n",
      "Epoch 77/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4673 - val_loss: 0.6608\n",
      "Epoch 78/100\n",
      "360/360 [==============================] - 0s 964us/step - loss: 0.4633 - val_loss: 0.6534\n",
      "Epoch 79/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4618 - val_loss: 0.6479\n",
      "Epoch 80/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4610 - val_loss: 0.6185\n",
      "Epoch 81/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.6080\n",
      "Epoch 82/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4687 - val_loss: 0.6512\n",
      "Epoch 83/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4566 - val_loss: 0.6192\n",
      "Epoch 84/100\n",
      "360/360 [==============================] - 0s 655us/step - loss: 0.4536 - val_loss: 0.6472\n",
      "Epoch 85/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4488 - val_loss: 0.6392\n",
      "Epoch 86/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4497 - val_loss: 0.6288\n",
      "Epoch 87/100\n",
      "360/360 [==============================] - 0s 939us/step - loss: 0.4475 - val_loss: 0.6341\n",
      "Epoch 88/100\n",
      "360/360 [==============================] - 0s 925us/step - loss: 0.4608 - val_loss: 0.6311\n",
      "Epoch 89/100\n",
      "360/360 [==============================] - 0s 889us/step - loss: 0.4521 - val_loss: 0.6536\n",
      "Epoch 90/100\n",
      "360/360 [==============================] - 0s 917us/step - loss: 0.4569 - val_loss: 0.6118\n",
      "Epoch 91/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4462 - val_loss: 0.6197\n",
      "Epoch 92/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.6236\n",
      "Epoch 93/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4465 - val_loss: 0.6282\n",
      "Epoch 94/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4466 - val_loss: 0.6312\n",
      "Epoch 95/100\n",
      "360/360 [==============================] - 1s 2ms/step - loss: 0.4413 - val_loss: 0.6365\n",
      "Epoch 96/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4436 - val_loss: 0.6434\n",
      "Epoch 97/100\n",
      "360/360 [==============================] - 0s 992us/step - loss: 0.4391 - val_loss: 0.6146\n",
      "Epoch 98/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.6822\n",
      "Epoch 99/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4462 - val_loss: 0.6632\n",
      "Epoch 100/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.4462 - val_loss: 0.6258\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=2, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[3.02],\n",
       "       [3.54],\n",
       "       [3.14],\n",
       "       [2.28],\n",
       "       [4.75],\n",
       "       [1.83],\n",
       "       [2.33],\n",
       "       [1.99],\n",
       "       [4.06],\n",
       "       [2.71],\n",
       "       [2.81],\n",
       "       [2.83],\n",
       "       [4.2 ],\n",
       "       [3.13],\n",
       "       [1.3 ],\n",
       "       [1.49],\n",
       "       [2.47],\n",
       "       [3.5 ],\n",
       "       [3.17],\n",
       "       [1.82],\n",
       "       [3.84],\n",
       "       [3.04],\n",
       "       [4.06],\n",
       "       [2.14],\n",
       "       [4.32],\n",
       "       [1.23],\n",
       "       [3.13],\n",
       "       [2.  ],\n",
       "       [4.04],\n",
       "       [2.95],\n",
       "       [2.56],\n",
       "       [3.4 ],\n",
       "       [3.42],\n",
       "       [3.65],\n",
       "       [4.02],\n",
       "       [2.43],\n",
       "       [2.31],\n",
       "       [4.22],\n",
       "       [3.69],\n",
       "       [1.67],\n",
       "       [3.3 ],\n",
       "       [1.92],\n",
       "       [3.14],\n",
       "       [2.17],\n",
       "       [3.65],\n",
       "       [3.56],\n",
       "       [1.33],\n",
       "       [4.79],\n",
       "       [4.4 ],\n",
       "       [2.49],\n",
       "       [3.4 ],\n",
       "       [4.02],\n",
       "       [4.12],\n",
       "       [4.69],\n",
       "       [2.14],\n",
       "       [1.09],\n",
       "       [4.26],\n",
       "       [1.89],\n",
       "       [3.67],\n",
       "       [5.23],\n",
       "       [2.26],\n",
       "       [3.06],\n",
       "       [1.1 ],\n",
       "       [2.42],\n",
       "       [2.16],\n",
       "       [3.8 ],\n",
       "       [2.97],\n",
       "       [2.19],\n",
       "       [1.97],\n",
       "       [1.05],\n",
       "       [3.88],\n",
       "       [3.06],\n",
       "       [1.66],\n",
       "       [2.47],\n",
       "       [3.84],\n",
       "       [4.48],\n",
       "       [5.12],\n",
       "       [2.25],\n",
       "       [1.8 ],\n",
       "       [2.29],\n",
       "       [4.37],\n",
       "       [3.73],\n",
       "       [0.92],\n",
       "       [2.03],\n",
       "       [2.15],\n",
       "       [2.71],\n",
       "       [3.42],\n",
       "       [3.93],\n",
       "       [2.73],\n",
       "       [4.67],\n",
       "       [1.51],\n",
       "       [4.66],\n",
       "       [3.19],\n",
       "       [2.81],\n",
       "       [1.76],\n",
       "       [3.42],\n",
       "       [2.99],\n",
       "       [1.61],\n",
       "       [3.17],\n",
       "       [2.56],\n",
       "       [2.37],\n",
       "       [3.13],\n",
       "       [4.37],\n",
       "       [2.98],\n",
       "       [4.26],\n",
       "       [2.69],\n",
       "       [3.88],\n",
       "       [4.29],\n",
       "       [3.07],\n",
       "       [2.6 ],\n",
       "       [2.41],\n",
       "       [0.98],\n",
       "       [2.97],\n",
       "       [3.67],\n",
       "       [2.54],\n",
       "       [3.6 ],\n",
       "       [3.1 ],\n",
       "       [3.68],\n",
       "       [2.42],\n",
       "       [3.34],\n",
       "       [3.29],\n",
       "       [2.43],\n",
       "       [3.29],\n",
       "       [3.21],\n",
       "       [2.98],\n",
       "       [3.03],\n",
       "       [3.21],\n",
       "       [1.31],\n",
       "       [2.85],\n",
       "       [3.98],\n",
       "       [3.77],\n",
       "       [1.31],\n",
       "       [4.18],\n",
       "       [3.55],\n",
       "       [2.55],\n",
       "       [4.07],\n",
       "       [4.45],\n",
       "       [1.72],\n",
       "       [2.63],\n",
       "       [3.74],\n",
       "       [3.91],\n",
       "       [3.42],\n",
       "       [2.09],\n",
       "       [1.69],\n",
       "       [1.83],\n",
       "       [3.1 ],\n",
       "       [2.6 ],\n",
       "       [1.84],\n",
       "       [4.45],\n",
       "       [3.82],\n",
       "       [3.  ],\n",
       "       [3.64],\n",
       "       [3.  ],\n",
       "       [1.59],\n",
       "       [4.13],\n",
       "       [4.17],\n",
       "       [1.06],\n",
       "       [3.88],\n",
       "       [2.77],\n",
       "       [2.33],\n",
       "       [2.48],\n",
       "       [4.29],\n",
       "       [2.78],\n",
       "       [3.11],\n",
       "       [3.45],\n",
       "       [3.17],\n",
       "       [3.23],\n",
       "       [4.27],\n",
       "       [2.28],\n",
       "       [1.6 ],\n",
       "       [2.6 ],\n",
       "       [4.64],\n",
       "       [2.56],\n",
       "       [3.05],\n",
       "       [3.26],\n",
       "       [2.75],\n",
       "       [3.71],\n",
       "       [4.43],\n",
       "       [5.44],\n",
       "       [2.  ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "\n",
    "pred = model.predict(X_test)\n",
    "rounded_pred = np.around(pred, decimals=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.10006326], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "def pearsonr(x, y):\n",
    "  # Assume len(x) == len(y)\n",
    "  n = len(x)\n",
    "  sum_x = float(sum(x))\n",
    "  sum_y = float(sum(y))\n",
    "  sum_x_sq = sum(xi*xi for xi in x)\n",
    "  sum_y_sq = sum(yi*yi for yi in y)\n",
    "  psum = sum(xi*yi for xi, yi in zip(x, y))\n",
    "  num = psum - (sum_x * sum_y/n)\n",
    "  den = pow((sum_x_sq - pow(sum_x, 2) / n) * (sum_y_sq - pow(sum_y, 2) / n), 0.5)\n",
    "  if den == 0: return 0\n",
    "  return num / den\n",
    "pearsonr(rounded_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}