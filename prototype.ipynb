{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04c2c6958f73f61d7b8bffd4193a0d9584c9ef39256921fe54ce430734ddbdc1d",
   "display_name": "Python 3.9.4 64-bit ('QandU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "#own functions\n",
    "\n",
    "from test_train_devide import get_split, get_y_array\n",
    "from ML import train_model, test_model\n",
    "\n",
    "\n",
    "data_set = pd.read_excel(\"Dataset/\"+\"training.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  akkusativ  genitiv  num_words  longest_word_length  \\\n",
       "0        1          0        0         31                   22   \n",
       "1        1          0        0         11                   22   \n",
       "2        1          1        0         28                   22   \n",
       "3        1          0        0         24                   22   \n",
       "4        1          0        0         36                   22   \n",
       "..     ...        ...      ...        ...                  ...   \n",
       "894      1          0        1         23                   18   \n",
       "895      1          0        0         27                   18   \n",
       "896      1          1        0         29                   18   \n",
       "897      1          1        0         43                   18   \n",
       "898      1          0        1         32                   18   \n",
       "\n",
       "     shortest_word_length    target  \n",
       "0                       2  3.800000  \n",
       "1                       2  2.285714  \n",
       "2                       2  3.000000  \n",
       "3                       2  4.000000  \n",
       "4                       2  4.900000  \n",
       "..                    ...       ...  \n",
       "894                     2  3.777778  \n",
       "895                     2  3.375000  \n",
       "896                     2  4.058824  \n",
       "897                     2  5.111111  \n",
       "898                     2  5.000000  \n",
       "\n",
       "[899 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>akkusativ</th>\n      <th>genitiv</th>\n      <th>num_words</th>\n      <th>longest_word_length</th>\n      <th>shortest_word_length</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31</td>\n      <td>22</td>\n      <td>2</td>\n      <td>3.800000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>22</td>\n      <td>2</td>\n      <td>2.285714</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28</td>\n      <td>22</td>\n      <td>2</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24</td>\n      <td>22</td>\n      <td>2</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>22</td>\n      <td>2</td>\n      <td>4.900000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>23</td>\n      <td>18</td>\n      <td>2</td>\n      <td>3.777778</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>18</td>\n      <td>2</td>\n      <td>3.375000</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>29</td>\n      <td>18</td>\n      <td>2</td>\n      <td>4.058824</td>\n    </tr>\n    <tr>\n      <th>897</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>43</td>\n      <td>18</td>\n      <td>2</td>\n      <td>5.111111</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>32</td>\n      <td>18</td>\n      <td>2</td>\n      <td>5.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>899 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "target = []\n",
    "splited_words = []\n",
    "num_words = []\n",
    "num_letters_array = []\n",
    "longest_word_length = []\n",
    "shortest_word_length = []\n",
    "genitiv = []\n",
    "akkusativ = []\n",
    "dativ =[]\n",
    "\n",
    "for x in range(max(data_set.index)):\n",
    "    current_target = data_set.loc[x,'MOS']\n",
    "    target.append(current_target)\n",
    "    current_sentence = data_set.loc[x,'Sentence']\n",
    "    splited_words.append(current_sentence.split())\n",
    "    num_words.append(len(splited_words[x]))\n",
    "    num_letters = []\n",
    "        \n",
    "    if \"des\" in current_sentence:\n",
    "        genitiv.append(1)\n",
    "    else:\n",
    "        genitiv.append(0)\n",
    "\n",
    "    if \"dem\" in current_sentence:\n",
    "        akkusativ.append(1)\n",
    "    else:\n",
    "        akkusativ.append(0)\n",
    "\n",
    "    if \"den\" in current_sentence:\n",
    "        dativ.append(1)\n",
    "    else:\n",
    "        dativ.append(1)\n",
    "\n",
    "    for y in range (num_words[x]):\n",
    "        current_word = splited_words[x][y]\n",
    "\n",
    "        num_letters.append(len(current_word))\n",
    "        num_letters_array.append(num_letters)\n",
    "\n",
    "    longest_word_length.append(max(num_letters_array[x])) \n",
    "    shortest_word_length.append(min(num_letters_array[x])) \n",
    "\n",
    "feature_dict = {\n",
    "    'dativ':dativ, \n",
    "    'akkusativ': akkusativ, \n",
    "    'genitiv': genitiv, \n",
    "    'num_words':num_words, \n",
    "    'longest_word_length':longest_word_length,\n",
    "    'shortest_word_length':shortest_word_length, \n",
    "    'target':target\n",
    "    }\n",
    "feature_dataframe = pd.DataFrame(data=feature_dict)\n",
    "feature_dataframe   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  genitiv\n",
       "366      1        0\n",
       "458      1        0\n",
       "625      1        1\n",
       "150      1        0\n",
       "245      1        0\n",
       "..     ...      ...\n",
       "483      1        0\n",
       "369      1        1\n",
       "229      1        0\n",
       "122      1        0\n",
       "310      1        0\n",
       "\n",
       "[719 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>genitiv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>366</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>625</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>483</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>310</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>719 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_dataframe.drop(['target'], axis=1)\n",
    "y = feature_dataframe['target']\n",
    "\n",
    "X_test, X_train,  y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "X_train[['dativ','genitiv']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-26-972b59225ed9>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train[['num_words', 'longest_word_length', 'shortest_word_length']] = scaler.fit_transform(X_train[['num_words', 'longest_word_length', 'shortest_word_length']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n<ipython-input-26-972b59225ed9>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test[['num_words', 'longest_word_length', 'shortest_word_length']] = scaler.transform(X_test[['num_words', 'longest_word_length', 'shortest_word_length']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  akkusativ  genitiv  num_words  longest_word_length  \\\n",
       "404      1          0        0  -0.889616            -1.212706   \n",
       "578      1          0        0  -0.226292             0.486560   \n",
       "258      1          0        1   1.763680             2.610642   \n",
       "90       1          0        1   1.100356            -1.212706   \n",
       "460      1          0        0   1.289877             1.123784   \n",
       "..     ...        ...      ...        ...                  ...   \n",
       "264      1          1        0  -0.036771             2.610642   \n",
       "55       1          0        0   0.342271             0.698968   \n",
       "125      1          0        0  -1.173897             1.336193   \n",
       "305      1          1        0  -0.700095            -0.575482   \n",
       "707      1          0        0  -1.173897             0.061743   \n",
       "\n",
       "     shortest_word_length  \n",
       "404             -0.197528  \n",
       "578             -0.197528  \n",
       "258             -0.197528  \n",
       "90               1.775009  \n",
       "460             -0.197528  \n",
       "..                    ...  \n",
       "264             -0.197528  \n",
       "55              -0.197528  \n",
       "125             -0.197528  \n",
       "305             -0.197528  \n",
       "707             -0.197528  \n",
       "\n",
       "[180 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>akkusativ</th>\n      <th>genitiv</th>\n      <th>num_words</th>\n      <th>longest_word_length</th>\n      <th>shortest_word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>404</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.889616</td>\n      <td>-1.212706</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.226292</td>\n      <td>0.486560</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>258</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.763680</td>\n      <td>2.610642</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.100356</td>\n      <td>-1.212706</td>\n      <td>1.775009</td>\n    </tr>\n    <tr>\n      <th>460</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.289877</td>\n      <td>1.123784</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>264</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.036771</td>\n      <td>2.610642</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.342271</td>\n      <td>0.698968</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.173897</td>\n      <td>1.336193</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.700095</td>\n      <td>-0.575482</td>\n      <td>-0.197528</td>\n    </tr>\n    <tr>\n      <th>707</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.173897</td>\n      <td>0.061743</td>\n      <td>-0.197528</td>\n    </tr>\n  </tbody>\n</table>\n<p>180 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['num_words', 'longest_word_length', 'shortest_word_length']] = scaler.fit_transform(X_train[['num_words', 'longest_word_length', 'shortest_word_length']])\n",
    "X_test[['num_words', 'longest_word_length', 'shortest_word_length']] = scaler.transform(X_test[['num_words', 'longest_word_length', 'shortest_word_length']])\n",
    "\n",
    "X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4863 - accuracy: 0.0000e+00\n",
      "Epoch 415/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4676 - accuracy: 0.0000e+00\n",
      "Epoch 416/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4897 - accuracy: 0.0000e+00\n",
      "Epoch 417/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4799 - accuracy: 0.0000e+00\n",
      "Epoch 418/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4930 - accuracy: 0.0000e+00\n",
      "Epoch 419/600\n",
      "36/36 [==============================] - 0s 429us/step - loss: 1.4864 - accuracy: 0.0000e+00\n",
      "Epoch 420/600\n",
      "36/36 [==============================] - 0s 429us/step - loss: 1.4869 - accuracy: 0.0000e+00\n",
      "Epoch 421/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4921 - accuracy: 0.0000e+00\n",
      "Epoch 422/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4760 - accuracy: 0.0000e+00\n",
      "Epoch 423/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.5050 - accuracy: 0.0000e+00\n",
      "Epoch 424/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4769 - accuracy: 0.0000e+00\n",
      "Epoch 425/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4774 - accuracy: 0.0000e+00\n",
      "Epoch 426/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4861 - accuracy: 0.0000e+00\n",
      "Epoch 427/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4818 - accuracy: 0.0000e+00\n",
      "Epoch 428/600\n",
      "36/36 [==============================] - 0s 429us/step - loss: 1.4669 - accuracy: 0.0000e+00\n",
      "Epoch 429/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4660 - accuracy: 0.0000e+00\n",
      "Epoch 430/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4751 - accuracy: 0.0000e+00\n",
      "Epoch 431/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4796 - accuracy: 0.0000e+00\n",
      "Epoch 432/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4880 - accuracy: 0.0000e+00\n",
      "Epoch 433/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4712 - accuracy: 0.0000e+00\n",
      "Epoch 434/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4819 - accuracy: 0.0000e+00\n",
      "Epoch 435/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4635 - accuracy: 0.0000e+00\n",
      "Epoch 436/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4635 - accuracy: 0.0000e+00\n",
      "Epoch 437/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4756 - accuracy: 0.0000e+00\n",
      "Epoch 438/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4641 - accuracy: 0.0000e+00\n",
      "Epoch 439/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4694 - accuracy: 0.0000e+00\n",
      "Epoch 440/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4679 - accuracy: 0.0000e+00\n",
      "Epoch 441/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4828 - accuracy: 0.0000e+00\n",
      "Epoch 442/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4931 - accuracy: 0.0000e+00\n",
      "Epoch 443/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.5099 - accuracy: 0.0000e+00\n",
      "Epoch 444/600\n",
      "36/36 [==============================] - 0s 429us/step - loss: 1.4692 - accuracy: 0.0000e+00\n",
      "Epoch 445/600\n",
      "36/36 [==============================] - 0s 508us/step - loss: 1.4679 - accuracy: 0.0000e+00\n",
      "Epoch 446/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4666 - accuracy: 0.0000e+00\n",
      "Epoch 447/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4781 - accuracy: 0.0000e+00\n",
      "Epoch 448/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4677 - accuracy: 0.0000e+00\n",
      "Epoch 449/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4833 - accuracy: 0.0000e+00\n",
      "Epoch 450/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4657 - accuracy: 0.0000e+00\n",
      "Epoch 451/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4830 - accuracy: 0.0000e+00\n",
      "Epoch 452/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4906 - accuracy: 0.0000e+00\n",
      "Epoch 453/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4974 - accuracy: 0.0000e+00\n",
      "Epoch 454/600\n",
      "36/36 [==============================] - 0s 429us/step - loss: 1.4854 - accuracy: 0.0000e+00\n",
      "Epoch 455/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4623 - accuracy: 0.0000e+00\n",
      "Epoch 456/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4602 - accuracy: 0.0000e+00\n",
      "Epoch 457/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4593 - accuracy: 0.0000e+00\n",
      "Epoch 458/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4693 - accuracy: 0.0000e+00\n",
      "Epoch 459/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4609 - accuracy: 0.0000e+00\n",
      "Epoch 460/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4796 - accuracy: 0.0000e+00\n",
      "Epoch 461/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4629 - accuracy: 0.0000e+00\n",
      "Epoch 462/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4689 - accuracy: 0.0000e+00\n",
      "Epoch 463/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4689 - accuracy: 0.0000e+00\n",
      "Epoch 464/600\n",
      "36/36 [==============================] - 0s 686us/step - loss: 1.4729 - accuracy: 0.0000e+00\n",
      "Epoch 465/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4738 - accuracy: 0.0000e+00\n",
      "Epoch 466/600\n",
      "36/36 [==============================] - 0s 657us/step - loss: 1.4781 - accuracy: 0.0000e+00\n",
      "Epoch 467/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4724 - accuracy: 0.0000e+00\n",
      "Epoch 468/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4765 - accuracy: 0.0000e+00\n",
      "Epoch 469/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4758 - accuracy: 0.0000e+00\n",
      "Epoch 470/600\n",
      "36/36 [==============================] - 0s 629us/step - loss: 1.4802 - accuracy: 0.0000e+00\n",
      "Epoch 471/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4733 - accuracy: 0.0000e+00\n",
      "Epoch 472/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4684 - accuracy: 0.0000e+00\n",
      "Epoch 473/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4608 - accuracy: 0.0000e+00\n",
      "Epoch 474/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4754 - accuracy: 0.0000e+00\n",
      "Epoch 475/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4596 - accuracy: 0.0000e+00\n",
      "Epoch 476/600\n",
      "36/36 [==============================] - 0s 714us/step - loss: 1.4586 - accuracy: 0.0000e+00\n",
      "Epoch 477/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4641 - accuracy: 0.0000e+00\n",
      "Epoch 478/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4682 - accuracy: 0.0000e+00\n",
      "Epoch 479/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4768 - accuracy: 0.0000e+00\n",
      "Epoch 480/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4742 - accuracy: 0.0000e+00\n",
      "Epoch 481/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4678 - accuracy: 0.0000e+00\n",
      "Epoch 482/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4772 - accuracy: 0.0000e+00\n",
      "Epoch 483/600\n",
      "36/36 [==============================] - 0s 684us/step - loss: 1.4743 - accuracy: 0.0000e+00\n",
      "Epoch 484/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4601 - accuracy: 0.0000e+00\n",
      "Epoch 485/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4596 - accuracy: 0.0000e+00\n",
      "Epoch 486/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4714 - accuracy: 0.0000e+00\n",
      "Epoch 487/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4803 - accuracy: 0.0000e+00\n",
      "Epoch 488/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4631 - accuracy: 0.0000e+00\n",
      "Epoch 489/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4660 - accuracy: 0.0000e+00\n",
      "Epoch 490/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4695 - accuracy: 0.0000e+00\n",
      "Epoch 491/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4574 - accuracy: 0.0000e+00\n",
      "Epoch 492/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4794 - accuracy: 0.0000e+00\n",
      "Epoch 493/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4737 - accuracy: 0.0000e+00\n",
      "Epoch 494/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4580 - accuracy: 0.0000e+00\n",
      "Epoch 495/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4579 - accuracy: 0.0000e+00\n",
      "Epoch 496/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4689 - accuracy: 0.0000e+00\n",
      "Epoch 497/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4545 - accuracy: 0.0000e+00\n",
      "Epoch 498/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4756 - accuracy: 0.0000e+00\n",
      "Epoch 499/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4655 - accuracy: 0.0000e+00\n",
      "Epoch 500/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4700 - accuracy: 0.0000e+00\n",
      "Epoch 501/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4693 - accuracy: 0.0000e+00\n",
      "Epoch 502/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4590 - accuracy: 0.0000e+00\n",
      "Epoch 503/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4586 - accuracy: 0.0000e+00\n",
      "Epoch 504/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4862 - accuracy: 0.0000e+00\n",
      "Epoch 505/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4712 - accuracy: 0.0000e+00\n",
      "Epoch 506/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4650 - accuracy: 0.0000e+00\n",
      "Epoch 507/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4568 - accuracy: 0.0000e+00\n",
      "Epoch 508/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4611 - accuracy: 0.0000e+00\n",
      "Epoch 509/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4575 - accuracy: 0.0000e+00\n",
      "Epoch 510/600\n",
      "36/36 [==============================] - 0s 507us/step - loss: 1.4615 - accuracy: 0.0000e+00\n",
      "Epoch 511/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4517 - accuracy: 0.0000e+00\n",
      "Epoch 512/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4656 - accuracy: 0.0000e+00\n",
      "Epoch 513/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4727 - accuracy: 0.0000e+00\n",
      "Epoch 514/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4558 - accuracy: 0.0000e+00\n",
      "Epoch 515/600\n",
      "36/36 [==============================] - 0s 629us/step - loss: 1.4523 - accuracy: 0.0000e+00\n",
      "Epoch 516/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4542 - accuracy: 0.0000e+00\n",
      "Epoch 517/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4656 - accuracy: 0.0000e+00\n",
      "Epoch 518/600\n",
      "36/36 [==============================] - 0s 429us/step - loss: 1.4547 - accuracy: 0.0000e+00\n",
      "Epoch 519/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4536 - accuracy: 0.0000e+00\n",
      "Epoch 520/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4569 - accuracy: 0.0000e+00\n",
      "Epoch 521/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4643 - accuracy: 0.0000e+00\n",
      "Epoch 522/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4633 - accuracy: 0.0000e+00\n",
      "Epoch 523/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4523 - accuracy: 0.0000e+00\n",
      "Epoch 524/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4707 - accuracy: 0.0000e+00\n",
      "Epoch 525/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4505 - accuracy: 0.0000e+00\n",
      "Epoch 526/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4734 - accuracy: 0.0000e+00\n",
      "Epoch 527/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4764 - accuracy: 0.0000e+00\n",
      "Epoch 528/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4636 - accuracy: 0.0000e+00\n",
      "Epoch 529/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4613 - accuracy: 0.0000e+00\n",
      "Epoch 530/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4884 - accuracy: 0.0000e+00\n",
      "Epoch 531/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4529 - accuracy: 0.0000e+00\n",
      "Epoch 532/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4426 - accuracy: 0.0000e+00\n",
      "Epoch 533/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4534 - accuracy: 0.0000e+00\n",
      "Epoch 534/600\n",
      "36/36 [==============================] - 0s 472us/step - loss: 1.4485 - accuracy: 0.0000e+00\n",
      "Epoch 535/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4574 - accuracy: 0.0000e+00\n",
      "Epoch 536/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4557 - accuracy: 0.0000e+00\n",
      "Epoch 537/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4574 - accuracy: 0.0000e+00\n",
      "Epoch 538/600\n",
      "36/36 [==============================] - 0s 629us/step - loss: 1.4559 - accuracy: 0.0000e+00\n",
      "Epoch 539/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4395 - accuracy: 0.0000e+00\n",
      "Epoch 540/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4552 - accuracy: 0.0000e+00\n",
      "Epoch 541/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4584 - accuracy: 0.0000e+00\n",
      "Epoch 542/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4420 - accuracy: 0.0000e+00\n",
      "Epoch 543/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4614 - accuracy: 0.0000e+00\n",
      "Epoch 544/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4453 - accuracy: 0.0000e+00\n",
      "Epoch 545/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4593 - accuracy: 0.0000e+00\n",
      "Epoch 546/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4453 - accuracy: 0.0000e+00\n",
      "Epoch 547/600\n",
      "36/36 [==============================] - 0s 429us/step - loss: 1.4525 - accuracy: 0.0000e+00\n",
      "Epoch 548/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4666 - accuracy: 0.0000e+00\n",
      "Epoch 549/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4646 - accuracy: 0.0000e+00\n",
      "Epoch 550/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4484 - accuracy: 0.0000e+00\n",
      "Epoch 551/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4545 - accuracy: 0.0000e+00\n",
      "Epoch 552/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4412 - accuracy: 0.0000e+00\n",
      "Epoch 553/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4393 - accuracy: 0.0000e+00\n",
      "Epoch 554/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4591 - accuracy: 0.0000e+00\n",
      "Epoch 555/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4628 - accuracy: 0.0000e+00\n",
      "Epoch 556/600\n",
      "36/36 [==============================] - 0s 657us/step - loss: 1.4698 - accuracy: 0.0000e+00\n",
      "Epoch 557/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4434 - accuracy: 0.0000e+00\n",
      "Epoch 558/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4573 - accuracy: 0.0000e+00\n",
      "Epoch 559/600\n",
      "36/36 [==============================] - 0s 629us/step - loss: 1.4690 - accuracy: 0.0000e+00\n",
      "Epoch 560/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4750 - accuracy: 0.0000e+00\n",
      "Epoch 561/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4560 - accuracy: 0.0000e+00\n",
      "Epoch 562/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4616 - accuracy: 0.0000e+00\n",
      "Epoch 563/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4534 - accuracy: 0.0000e+00\n",
      "Epoch 564/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4519 - accuracy: 0.0000e+00\n",
      "Epoch 565/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4550 - accuracy: 0.0000e+00\n",
      "Epoch 566/600\n",
      "36/36 [==============================] - 0s 686us/step - loss: 1.4458 - accuracy: 0.0000e+00\n",
      "Epoch 567/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4372 - accuracy: 0.0000e+00\n",
      "Epoch 568/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4724 - accuracy: 0.0000e+00\n",
      "Epoch 569/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4550 - accuracy: 0.0000e+00\n",
      "Epoch 570/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4380 - accuracy: 0.0000e+00\n",
      "Epoch 571/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4395 - accuracy: 0.0000e+00\n",
      "Epoch 572/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4435 - accuracy: 0.0000e+00\n",
      "Epoch 573/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4555 - accuracy: 0.0000e+00\n",
      "Epoch 574/600\n",
      "36/36 [==============================] - 0s 501us/step - loss: 1.4401 - accuracy: 0.0000e+00\n",
      "Epoch 575/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4551 - accuracy: 0.0000e+00\n",
      "Epoch 576/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4714 - accuracy: 0.0000e+00\n",
      "Epoch 577/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4480 - accuracy: 0.0000e+00\n",
      "Epoch 578/600\n",
      "36/36 [==============================] - 0s 572us/step - loss: 1.4513 - accuracy: 0.0000e+00\n",
      "Epoch 579/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4429 - accuracy: 0.0000e+00\n",
      "Epoch 580/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4499 - accuracy: 0.0000e+00\n",
      "Epoch 581/600\n",
      "36/36 [==============================] - 0s 743us/step - loss: 1.4539 - accuracy: 0.0000e+00\n",
      "Epoch 582/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4519 - accuracy: 0.0000e+00\n",
      "Epoch 583/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4437 - accuracy: 0.0000e+00\n",
      "Epoch 584/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4411 - accuracy: 0.0000e+00\n",
      "Epoch 585/600\n",
      "36/36 [==============================] - 0s 772us/step - loss: 1.4357 - accuracy: 0.0000e+00\n",
      "Epoch 586/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4556 - accuracy: 0.0000e+00\n",
      "Epoch 587/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4527 - accuracy: 0.0000e+00\n",
      "Epoch 588/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4447 - accuracy: 0.0000e+00\n",
      "Epoch 589/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4440 - accuracy: 0.0000e+00\n",
      "Epoch 590/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4522 - accuracy: 0.0000e+00\n",
      "Epoch 591/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4448 - accuracy: 0.0000e+00\n",
      "Epoch 592/600\n",
      "36/36 [==============================] - 0s 543us/step - loss: 1.4298 - accuracy: 0.0000e+00\n",
      "Epoch 593/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4370 - accuracy: 0.0000e+00\n",
      "Epoch 594/600\n",
      "36/36 [==============================] - 0s 600us/step - loss: 1.4444 - accuracy: 0.0000e+00\n",
      "Epoch 595/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4385 - accuracy: 0.0000e+00\n",
      "Epoch 596/600\n",
      "36/36 [==============================] - 0s 486us/step - loss: 1.4434 - accuracy: 0.0000e+00\n",
      "Epoch 597/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4393 - accuracy: 0.0000e+00\n",
      "Epoch 598/600\n",
      "36/36 [==============================] - 0s 457us/step - loss: 1.4367 - accuracy: 0.0000e+00\n",
      "Epoch 599/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4406 - accuracy: 0.0000e+00\n",
      "Epoch 600/600\n",
      "36/36 [==============================] - 0s 514us/step - loss: 1.4528 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e1bac09910>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and multilabel-indicator targets",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-2ba37dbd46d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[0;32m     93\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_predict = model.predict(X_test)\n",
    "y_predict\n",
    "\n",
    "acc = accuracy_score(y_test, y_predict_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}