{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04c2c6958f73f61d7b8bffd4193a0d9584c9ef39256921fe54ce430734ddbdc1d",
   "display_name": "Python 3.9.4 64-bit ('QandU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      Weitergehende Sicherungsmaßnahmen können eine ...\n",
       "1      In der großen Kasernenanlage im Norden Kiels k...\n",
       "2      Premierminister David Lloyd George honorierte ...\n",
       "3      Der Beitrag der Truppen dieser Dominions währe...\n",
       "4      Eine Balance zwischen verschiedenen Lebensbere...\n",
       "                             ...                        \n",
       "895    Zwischen 1901 und 2010 ist er um ca 1,7 cm pro...\n",
       "896    Aus Furcht vor einem Bürgerkrieg wollte sie – ...\n",
       "897    In den meisten Politikfeldern gilt dafür seit ...\n",
       "898    Aufgrund der Wärmekapazität des Gesteins, und ...\n",
       "899    Die Klinge ist zumeist aus nicht rostfreiem Ko...\n",
       "Name: Sentence, Length: 900, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# load data and write out sentence and target\n",
    "import pandas as pd\n",
    "\n",
    "loaded_set = pd.read_excel(\"Dataset/\"+\"training.xlsx\")\n",
    "\n",
    "#x_train, x_test,  y_train, y_test = train_test_split(loaded_set['Sentence'], loaded_set['MOS'], test_size=.2)\n",
    "loaded_set['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  akkusativ  genitiv  num_words  letter_count  \\\n",
       "0        1          0        0         31           231   \n",
       "1        1          0        0         11            61   \n",
       "2        1          1        0         28           204   \n",
       "3        1          0        0         24           139   \n",
       "4        1          0        0         36           297   \n",
       "..     ...        ...      ...        ...           ...   \n",
       "895      1          0        0         27           114   \n",
       "896      1          1        0         29           180   \n",
       "897      1          1        0         43           285   \n",
       "898      1          0        1         32           211   \n",
       "899      1          0        0         12            81   \n",
       "\n",
       "     avarange_letter_per_word  longest_word_length  shortest_word_length  \n",
       "0                    7.451613                   22                     2  \n",
       "1                    5.545455                   16                     2  \n",
       "2                    7.285714                   20                     2  \n",
       "3                    5.791667                   11                     3  \n",
       "4                    8.250000                   23                     2  \n",
       "..                        ...                  ...                   ...  \n",
       "895                  4.222222                   10                     2  \n",
       "896                  6.206897                   14                     1  \n",
       "897                  6.627907                   22                     2  \n",
       "898                  6.593750                   15                     3  \n",
       "899                  6.750000                   16                     3  \n",
       "\n",
       "[900 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>akkusativ</th>\n      <th>genitiv</th>\n      <th>num_words</th>\n      <th>letter_count</th>\n      <th>avarange_letter_per_word</th>\n      <th>longest_word_length</th>\n      <th>shortest_word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31</td>\n      <td>231</td>\n      <td>7.451613</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>61</td>\n      <td>5.545455</td>\n      <td>16</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28</td>\n      <td>204</td>\n      <td>7.285714</td>\n      <td>20</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24</td>\n      <td>139</td>\n      <td>5.791667</td>\n      <td>11</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>297</td>\n      <td>8.250000</td>\n      <td>23</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>114</td>\n      <td>4.222222</td>\n      <td>10</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>29</td>\n      <td>180</td>\n      <td>6.206897</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>897</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>43</td>\n      <td>285</td>\n      <td>6.627907</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>32</td>\n      <td>211</td>\n      <td>6.593750</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>899</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>81</td>\n      <td>6.750000</td>\n      <td>16</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "letter_count = []\n",
    "avarange_letter_per_word = []\n",
    "num_words = []\n",
    "num_letters_array = []\n",
    "longest_word_length = []\n",
    "shortest_word_length = []\n",
    "genitiv = []\n",
    "akkusativ = []\n",
    "dativ =[]\n",
    "\n",
    "for sen in loaded_set['Sentence']:\n",
    "    current_sen_split = sen.split()\n",
    "    num_words.append(len(current_sen_split))\n",
    "    num_letters = []\n",
    "        \n",
    "    if \"des\" in sen:\n",
    "        genitiv.append(1)\n",
    "    else:\n",
    "        genitiv.append(0)\n",
    "\n",
    "    if \"dem\" in sen:\n",
    "        akkusativ.append(1)\n",
    "    else:\n",
    "        akkusativ.append(0)\n",
    "\n",
    "    if \"den\" in sen:\n",
    "        dativ.append(1)\n",
    "    else:\n",
    "        dativ.append(1)\n",
    "\n",
    "    for y in range(len(current_sen_split)):\n",
    "        current_word = current_sen_split[y]\n",
    "   \n",
    "        num_letters.append(len(current_word))\n",
    "   \n",
    "    current_lettercount = sum(num_letters)\n",
    "    letter_count.append(current_lettercount) \n",
    "    avarange_letter_per_word.append(current_lettercount/len(current_sen_split))\n",
    "    longest_word_length.append(max(num_letters)) \n",
    "    shortest_word_length.append(min(num_letters)) \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "feature_dict = {\n",
    "    'dativ':dativ, \n",
    "    'akkusativ': akkusativ, \n",
    "    'genitiv': genitiv, \n",
    "    'num_words':num_words,\n",
    "    'letter_count':letter_count,\n",
    "    'avarange_letter_per_word':avarange_letter_per_word,\n",
    "    'longest_word_length':longest_word_length,\n",
    "    'shortest_word_length':shortest_word_length, \n",
    "    }\n",
    "feature_dataframe = pd.DataFrame(data=feature_dict)\n",
    "feature_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_dataframe\n",
    "y = round(loaded_set['MOS'],2)\n",
    "\n",
    "X_train, X_test,  y_train, y_test = train_test_split(X, y, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-99-b7800ab94972>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.fit_transform(X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n<ipython-input-99-b7800ab94972>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.transform(X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  akkusativ  genitiv  num_words  letter_count  \\\n",
       "10       1          0        0  -0.559035     -0.391342   \n",
       "749      1          0        0   0.023883      0.141297   \n",
       "726      1          0        0   0.703955      0.673937   \n",
       "643      1          0        0   0.703955      1.373977   \n",
       "619      1          1        0   0.995414      1.328322   \n",
       "..     ...        ...      ...        ...           ...   \n",
       "554      1          0        0  -1.141954     -1.258783   \n",
       "204      1          0        0  -0.753341     -0.710925   \n",
       "520      1          0        0  -0.947648     -1.060945   \n",
       "683      1          0        0   0.315343      0.628282   \n",
       "839      1          0        0  -0.753341     -0.969636   \n",
       "\n",
       "     avarange_letter_per_word  longest_word_length  shortest_word_length  \n",
       "10                   0.705978            -0.123628             -0.314986  \n",
       "749                  0.320837             0.348586             -0.314986  \n",
       "726                 -0.123957             0.584693             -0.314986  \n",
       "643                  1.546287             1.293014             -0.314986  \n",
       "619                  0.663963             1.293014             -0.314986  \n",
       "..                        ...                  ...                   ...  \n",
       "554                 -1.027159            -1.068057              1.238370  \n",
       "204                  0.157443             1.529121             -0.314986  \n",
       "520                 -0.806578            -0.831949             -0.314986  \n",
       "683                  0.821673            -0.831949             -0.314986  \n",
       "839                 -1.231401            -0.831949             -0.314986  \n",
       "\n",
       "[180 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>akkusativ</th>\n      <th>genitiv</th>\n      <th>num_words</th>\n      <th>letter_count</th>\n      <th>avarange_letter_per_word</th>\n      <th>longest_word_length</th>\n      <th>shortest_word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.559035</td>\n      <td>-0.391342</td>\n      <td>0.705978</td>\n      <td>-0.123628</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.023883</td>\n      <td>0.141297</td>\n      <td>0.320837</td>\n      <td>0.348586</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>726</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.703955</td>\n      <td>0.673937</td>\n      <td>-0.123957</td>\n      <td>0.584693</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>643</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.703955</td>\n      <td>1.373977</td>\n      <td>1.546287</td>\n      <td>1.293014</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>619</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.995414</td>\n      <td>1.328322</td>\n      <td>0.663963</td>\n      <td>1.293014</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>554</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1.141954</td>\n      <td>-1.258783</td>\n      <td>-1.027159</td>\n      <td>-1.068057</td>\n      <td>1.238370</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.753341</td>\n      <td>-0.710925</td>\n      <td>0.157443</td>\n      <td>1.529121</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.947648</td>\n      <td>-1.060945</td>\n      <td>-0.806578</td>\n      <td>-0.831949</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>683</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.315343</td>\n      <td>0.628282</td>\n      <td>0.821673</td>\n      <td>-0.831949</td>\n      <td>-0.314986</td>\n    </tr>\n    <tr>\n      <th>839</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.753341</td>\n      <td>-0.969636</td>\n      <td>-1.231401</td>\n      <td>-0.831949</td>\n      <td>-0.314986</td>\n    </tr>\n  </tbody>\n</table>\n<p>180 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.fit_transform(X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\n",
    "X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.transform(X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\n",
    "\n",
    "X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.InputLayer(8),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(120, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "360/360 [==============================] - 0s 727us/step - loss: 0.5730 - val_loss: 0.6736\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=2, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2.75],\n",
       "       [2.98],\n",
       "       [3.34],\n",
       "       [3.86],\n",
       "       [3.66],\n",
       "       [3.64],\n",
       "       [2.11],\n",
       "       [2.33],\n",
       "       [4.6 ],\n",
       "       [4.36],\n",
       "       [2.37],\n",
       "       [2.78],\n",
       "       [5.52],\n",
       "       [2.66],\n",
       "       [2.77],\n",
       "       [4.14],\n",
       "       [2.99],\n",
       "       [3.87],\n",
       "       [1.88],\n",
       "       [2.23],\n",
       "       [1.51],\n",
       "       [1.87],\n",
       "       [3.81],\n",
       "       [1.36],\n",
       "       [3.98],\n",
       "       [3.51],\n",
       "       [3.01],\n",
       "       [4.71],\n",
       "       [4.65],\n",
       "       [7.01],\n",
       "       [3.9 ],\n",
       "       [2.53],\n",
       "       [0.98],\n",
       "       [3.94],\n",
       "       [2.97],\n",
       "       [2.29],\n",
       "       [2.84],\n",
       "       [1.28],\n",
       "       [3.11],\n",
       "       [3.97],\n",
       "       [5.39],\n",
       "       [3.01],\n",
       "       [2.69],\n",
       "       [1.17],\n",
       "       [4.  ],\n",
       "       [3.78],\n",
       "       [4.14],\n",
       "       [4.47],\n",
       "       [4.52],\n",
       "       [4.28],\n",
       "       [6.24],\n",
       "       [3.12],\n",
       "       [5.88],\n",
       "       [3.15],\n",
       "       [3.03],\n",
       "       [2.62],\n",
       "       [3.54],\n",
       "       [4.34],\n",
       "       [3.02],\n",
       "       [4.81],\n",
       "       [2.16],\n",
       "       [4.54],\n",
       "       [5.07],\n",
       "       [2.75],\n",
       "       [2.66],\n",
       "       [2.96],\n",
       "       [3.09],\n",
       "       [2.88],\n",
       "       [1.74],\n",
       "       [4.8 ],\n",
       "       [4.37],\n",
       "       [5.61],\n",
       "       [1.13],\n",
       "       [2.12],\n",
       "       [2.13],\n",
       "       [3.72],\n",
       "       [3.73],\n",
       "       [2.77],\n",
       "       [2.58],\n",
       "       [2.33],\n",
       "       [3.8 ],\n",
       "       [2.6 ],\n",
       "       [3.25],\n",
       "       [3.53],\n",
       "       [2.72],\n",
       "       [3.22],\n",
       "       [3.24],\n",
       "       [2.41],\n",
       "       [6.34],\n",
       "       [2.78],\n",
       "       [3.32],\n",
       "       [2.95],\n",
       "       [3.39],\n",
       "       [3.63],\n",
       "       [4.87],\n",
       "       [3.1 ],\n",
       "       [1.71],\n",
       "       [2.89],\n",
       "       [3.28],\n",
       "       [3.17],\n",
       "       [4.46],\n",
       "       [2.63],\n",
       "       [4.18],\n",
       "       [2.56],\n",
       "       [1.4 ],\n",
       "       [2.12],\n",
       "       [2.96],\n",
       "       [2.79],\n",
       "       [3.53],\n",
       "       [2.57],\n",
       "       [2.8 ],\n",
       "       [2.3 ],\n",
       "       [4.38],\n",
       "       [3.01],\n",
       "       [2.75],\n",
       "       [2.4 ],\n",
       "       [3.05],\n",
       "       [2.74],\n",
       "       [2.64],\n",
       "       [5.22],\n",
       "       [1.54],\n",
       "       [1.15],\n",
       "       [4.09],\n",
       "       [5.89],\n",
       "       [2.93],\n",
       "       [2.71],\n",
       "       [3.67],\n",
       "       [3.41],\n",
       "       [3.75],\n",
       "       [4.05],\n",
       "       [2.43],\n",
       "       [2.23],\n",
       "       [1.26],\n",
       "       [3.13],\n",
       "       [2.95],\n",
       "       [2.46],\n",
       "       [2.76],\n",
       "       [2.71],\n",
       "       [2.84],\n",
       "       [2.87],\n",
       "       [2.32],\n",
       "       [4.51],\n",
       "       [3.27],\n",
       "       [2.94],\n",
       "       [4.33],\n",
       "       [5.62],\n",
       "       [3.3 ],\n",
       "       [3.2 ],\n",
       "       [2.74],\n",
       "       [2.96],\n",
       "       [3.74],\n",
       "       [2.97],\n",
       "       [3.25],\n",
       "       [4.62],\n",
       "       [1.26],\n",
       "       [4.24],\n",
       "       [2.57],\n",
       "       [4.29],\n",
       "       [2.7 ],\n",
       "       [3.53],\n",
       "       [5.83],\n",
       "       [2.96],\n",
       "       [2.8 ],\n",
       "       [2.35],\n",
       "       [2.77],\n",
       "       [3.14],\n",
       "       [2.02],\n",
       "       [2.05],\n",
       "       [3.29],\n",
       "       [4.03],\n",
       "       [3.06],\n",
       "       [3.17],\n",
       "       [2.32],\n",
       "       [2.42],\n",
       "       [3.12],\n",
       "       [1.07],\n",
       "       [2.36],\n",
       "       [1.64],\n",
       "       [3.56],\n",
       "       [1.59]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "\n",
    "pred = model.predict(X_test)\n",
    "rounded_pred = np.around(pred, decimals=2)\n",
    "y_test\n",
    "rounded_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}